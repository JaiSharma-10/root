-------------------------------------------------------------SNF

Databases and schemas are used to organize data stored in Snowflake:

A database is a logical grouping of schemas. Each database belongs to a single Snowflake account.

A schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.

Databases are used to group datasets (tables) together. A second-level organizational grouping, within a database, is called a schema. Every time you create a database, Snowflake will automatically create two schemas for you.

The INFORMATION_SCHEMA schema holds a collection of views.  The INFORMATION_SCHEMA schema cannot be deleted (dropped), renamed, or moved.

The PUBLIC schema is created empty and you can fill it with tables, views and other things over time. The PUBLIC schema can be dropped, renamed, or moved at any time.


Identity vs ACCESS
Username password one time Vs role base continuous
Authentication vs Authorization
 
Roles
--5 Roles
ACCOUNTADMIN  - Master of ACCOUNT
SYSADMIN	  - Master of creation of DB and schemas

Role-based Access Control(RBAC)
Discretionary Access Control (DAC). -you own what you create
SNF uses both RABC and DAC


--ROLE CREATION AS INHERITANCE
ACCOUNTADMIN --parent
SECURITYADMIN and SYSADMIN --child

ACCOUNTADMIN -it can manage all aspect of account
	SECURITYADMIN -security
	SYSADMIN -it can manage and create database and warehouse
	USERADMIN -it can manage users


-------------------------------------------------------------------------SNF

IN SNF WAREHOUSE IS NOT PLACE TO STORE DATA , IT IS COMPUTE RESOURCE, OR COMPUTING POWER. WAREHOUSE IS THE MACHINE THAT CRUCHES DATA , BIG MACHINE CAN CRUCH THAT REALLY
FAST LIKE WH-XL

============================================================================

Data warehouses vs Databases

Data warehouses
Store data from multiple sources in a fixed schema for long-term storage and reporting. They are useful for analytics, reporting, and read-heavy operations. Data warehouses can include current and historical data, and they allow business analysts and data scientists to analyze the data. They can also help companies make analytical queries to track variables for business intelligence. Data warehouses are well-suited for OLAP solutions, which can aggregate data for large-scale analytics. 

Databases
Store structured data in tables for short-term data storage and data manipulation tasks. They are ideal for transactional data and applications that require frequent read/write operations. Databases are mainly used for data manipulation and retrieval tasks, and they serve as a management system to organize and process data. They are also good for OLTP solutions, which provide fast data access

--SNOWFLAKE AND ITS BASIC SERVICE -DATA WAREHOUSING

Inside Snowflake, the virtual warehouse is a cluster of compute resources. It provides resources — including memory, temporary storage and CPU — to perform tasks such as DML operation and SQL execution.


--WHAT IS SNOWFLAKE?


Snowflake is a cloud-based data platform that offers data warehousing as its core service. Every Snowflake customer gains access to a dedicated virtual warehouse, which they build based on their storage and processing needs. After that, they migrate their data to the warehouse and implement a new data architecture, which results in all data pipelines leading to the central data repository.

Some of the features of the Snowflake data warehouse include:

Scalability – 
	Snowflake uses massively parallel processing (MPP) architecture, which distributes data across a cluster of independently running machines. This allows the warehouse to scale as needed, multiple times a day. When you have multiple users batch processing or stream processing large volumes of data simultaneously, the platform scales out and dedicates additional resources to you. It scales back down automatically afterward.
	
Built-in security features – 
	There are several security measures built into the platform, such as multi-factor authentication for all users, end-to-end encryption of data, and IP whitelisting.
	
Multi-cloud deployment – 
	The warehouse can be deployed on AWSopens in new tab, Azureopens in new tab, and Google Cloudopens in new tab.
--Grant ownership for all

--After cloning the database, transfer ownership to another role using the GRANT OWNERSHIP (see also example) function using COPY CURRENT GRANTS clause, for example

GRANT OWNERSHIP ON DATABASE mydb TO ROLE developer COPY CURRENT GRANTS;  
GRANT OWNERSHIP ON ALL SCHEMAS IN DATABASE mydb TO ROLE developer COPY CURRENT GRANTS;
GRANT OWNERSHIP ON ALL TABLES IN DATABASE mydb TO ROLE developer COPY CURRENT GRANTS; 
GRANT OWNERSHIP ON ALL VIEWS IN DATABASE mydb TO ROLE developer COPY CURRENT GRANTS;

create DATABASE GARDEN_PLANTS;

drop schema GARDEN_PLANTS.PUBLIC;

--VEGGIES, FRUITS and FLOWERS

create schema GARDEN_PLANTS.VEGGIES;
create schema GARDEN_PLANTS.FRUITS;
create schema GARDEN_PLANTS.FLOWERS;

describe database GARDEN_PLANTS;

show databases;
show schemas;

SHOW SCHEMAS IN ACCOUNT;


------------------------------------------------SNF
	
DATA MODELING and NORMALIZATION

--DATA MODELING

Data modeling is the process of organizing and mapping data using simplified diagrams, symbols, and text to represent data associations and flow.

Types of Approaches
There are four primary approaches to data modeling.  

1. Hierarchical
	A hierarchical database model organizes data into tree-like structures with data stored as interconnected records with one-to-many arrangements. Hierarchical
	database models are standard in XML and GIS.  

2. Relational
	A relational data model, AKA a relational model, manages data by providing methodology for specifying data and queries. Most relational data models use SQL for 
	data definition and query language.

3. Entity-relationship
	Entity-relationship models use diagrams to portray data and their relationships. Integrated with relational data models, entity-relationship models graphically 
	depict data elements to understand underlying models. 

4. Graph
	Graph data models are visualizations of complex relationships within data sets that are limited by a chosen domain.


Types of Data Models
There are three primary types of data models. 

1. Conceptual, defining what data system contains, used to organize, scope, and define business concepts and rules.

2. Logical, defining how a data system should be implemented, used to develop a technical map of rules and data structures.

3. Physical, defining how the data system will be implemented according to the specific use case.


--WHAT IS DATA NORMALIZATION?

Data normalization applies a set of formal rules to develop standardized, organized data, and eliminates data anomalies that cause difficulty for analysis. The clean data 
can then be easily grouped, understood, and interpreted. Without normalization, valuable data will go unused. 

Depending on your use case, data normalization may happen prior to or after loading the data into your data warehouse or platform. Some platforms, such as Snowflake, allow 
complete flexibility so you can store massive amounts of raw data and normalize only the data you need as you need it.

--DATA NORMALIZATION RULES
Data normalization rules are sequential—as you move through each rule, you normalize the data further. For this reason, you can think of normalization rules as “levels” of
normalization. Although there are five rules in total, only three are commonly used for practical reasons, since too much normalization results in inflexibility in the 
data model. 

--1NF (First Normal Form) rule (Remove duplicate)
	The first rule is about ensuring there are no repeating entries in a group. All attributes must have a unique name, and entities may consist of only two dimensions.
(You’ll need to create a new entity for additional dimensions.) Each entry must have only a single value for each cell, and each record must be unique. The goal of this 
first rule is to make it easier to search the data.

--2NF (Second Normal Form) rule (Unique primary key by segregating table further)
	The second rule is designed to eliminate redundant data. Before applying the 2NF rule, you must be sure that the 1NF rule has been fully applied. Data that is in 
the 2NF state will have only one primary key. For this reason, you must separate data by placing all subsets of data that can be placed in multiple rows into separate 
tables. Relationships can then be created via foreign key labels.

--3NF (Third Normal Form) rule (remove transitive)
	The third rule eliminates transitive dependency. As before, data must have achieved 1NF and 2NF status before you can apply the 3NF rule. Transitive dependency is
when a nonprime attribute depends on other nonprime attributes instead of depending on the prime attributes or primary key. So the third rule ensures that no attribute 
within an entity is dependent on a nonprime attribute that depends on the primary key. For this reason, if the primary key is changed, all data impacted by the change has 
to be put into a new table with a new foreign key.

------------------------------------------------------------------SNF
GETTING ROWS OF DATA INTO TABLE

Using an INSERT statement from the Worksheet. 
Using the Load Data Wizard.
Using COPY INTO statements. 

------------------------------------------------------------------SNF
--If Warehouse in SNF is NOT place to store data then
--WHERE DOES DATA IS STORED IN SNOWFLAKE?


Database Storage
When data is loaded into Snowflake, Snowflake reorganizes that data into its internal optimized, compressed, columnar format. Snowflake stores this optimized data in cloud
storage.

--Elaborate

Snowflake, a cloud-based data warehousing platform, stores data in cloud storage after reorganizing it into a compressed, COLUMNAR FORMAT. Snowflake manages all aspects of 
data storage, including organization, file size, structure, compression, metadata, and statistics.
 
Snowflake storage layer uses scalable CLOUD BLOB STORAGE to store data, tables, and query results. The storage layer is designed to scale independently from compute 
resources, and customers can increase and reduce storage and analytics requirements independently. 

Snowflake stores data in different places depending on the version of Snowflake: 
AWS version: Stores data on S3 
Azure version: Stores data on Azure Blob 

Snowflake stores all data in database tables, which are logically structured as collections of columns and rows. Within each micro-partition, data is stored in a columnar 
data structure, allowing better compression and efficient access only to those columns required by a query. 
Customers can only access the data objects stored by Snowflake through SQL query operations run using Snowflake. 

-------------------------------------------------------SNF

--DEFINING "WAREHOUSE" IN SNOWFLAKE:

People who have been working with data for awhile might think of the term "Data Warehouse" as referring to a special collection of data structures, but in Snowflake, warehouses do not store data.
In Snowflake, Warehouses are "workforces" -- they are used to perform the processing of data. 

Warehouse in SNF is COMPUTING RESOURCE 

When you create a Warehouse in Snowflake, you are defining a "workforce."

Teams are CLUSTERS, Team Members are SERVERS: 

The workforce of each warehouse is a team. A small warehouse has a small team, but just one team. An extra-large warehouse has a large team, but just one team.  
Snowflake Warehouse Sizes like eXtra-Small, Small, Medium, etc. all have one cluster. A small warehouse has one cluster made up of just a few servers. A larger warehouse has one cluster, made up of more servers.
Scaling Up and Down: 
Changing the size of warehouse changes the number of servers in the cluster. 
Changing the size of an existing warehouse is called scaling up or scaling down.

Scaling In and Out: 

If multi-cluster/elastic warehousing is available (Enterprise edition or above) a warehouse is capable of scaling out in times of increased demand.  (Adding temporary teams, made up of a collection of temporary workers). 
If multi-cluster scaling out takes place, clusters are added for the period of demand and then clusters are removed (snap back) when demand decreases. (Removing temporary teams). 
The number of servers in the original cluster dictates the number of servers in each cluster during periods where the warehouse scales out by adding clusters. 


--------------------------------------------------------SNF

when an xs warehouse automatically adds cluster to handle an increase workload, this  is example of scaling OUT.
Opposite of scaling out is snapping back.

Cluster is the group of servers.

All warehouse have single cluster until scaling out.

Number of server is only thing that changes.

--------------------------------------------------------snf
--api_integration

use role accountadmin;

create or replace api integration dora_api_integration
api_provider = aws_api_gateway
api_aws_role_arn = 'arn:aws:iam::321463406630:role/snowflakeLearnerAssumedRole'
enabled = true
api_allowed_prefixes = ('https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora');


--create grader function 
use role accountadmin;  

create or replace external function util_db.public.grader(
      step varchar
    , passed boolean
    , actual integer
    , expected integer
    , description varchar)
returns variant
api_integration = dora_api_integration 
context_headers = (current_timestamp, current_account, current_statement, current_account_name) 
as 'https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora/grader'

--------------------------------------------------------------------------
----------------------------------------snf
To COPY INTO statement, it is best to have 4 things in place:

A table 
A stage object
A file
A file format 
The file format is sort of optional, but it is a cleaner process if you have one, and we do!

copy into my_table_name
from @my_internal_stage
files = ( 'IF_I_HAD_A_FILE_LIKE_THIS.txt')
file_format = ( format_name='EXAMPLE_FILEFORMAT' );

-------------

HOW TO CREATE STAGE AND FILE FORMAT

--create table

create or replace table vegetable_details_soil_type
( plant_name varchar(25)
 ,soil_type number(1,0)
);

--create format 

create file format garden_plants.veggies.PIPECOLSEP_ONEHEADROW 
    type = 'CSV'--csv is used for any flat file (tsv, pipe-separated, etc)
    field_delimiter = '|' --pipes as column separators
    skip_header = 1 --one header row to skip
	
-- copy into

copy into vegetable_details_soil_type
from @util_db.public.my_internal_stage
files = ( 'VEG_NAME_TO_SOIL_TYPE_PIPE.txt')
file_format = ( format_name=GARDEN_PLANTS.VEGGIES.PIPECOLSEP_ONEHEADROW );


----------------------------------------------

--The data in the file, with no FILE FORMAT specified
select $1
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv;

--Same file but with one of the file formats we created earlier  
select $1, $2, $3
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv
(file_format => garden_plants.veggies.COMMASEP_DBLQUOT_ONEHEADROW);

--Same file but with the other file format we created earlier
select $1, $2, $3
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv
(file_format => garden_plants.veggies.PIPECOLSEP_ONEHEADROW );

---------------------------------------------

--CREATE FILE FORMAT
Creates a named file format that describes a set of staged data to access or load into Snowflake tables.
--query syntax
CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY | VOLATILE } ] FILE FORMAT [ IF NOT EXISTS ] <name>
  [ TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM} [ formatTypeOptions ] ]
  [ COMMENT = '<string_literal>' ]
  
 formatTypeOptions ::=
-- If TYPE = CSV
     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     RECORD_DELIMITER = '<character>' | NONE
     FIELD_DELIMITER = '<character>' | NONE
     FILE_EXTENSION = '<string>'
     PARSE_HEADER = TRUE | FALSE
     SKIP_HEADER = <integer>
     SKIP_BLANK_LINES = TRUE | FALSE
     DATE_FORMAT = '<string>' | AUTO
     TIME_FORMAT = '<string>' | AUTO
     TIMESTAMP_FORMAT = '<string>' | AUTO
     BINARY_FORMAT = HEX | BASE64 | UTF8
     ESCAPE = '<character>' | NONE
     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE
     TRIM_SPACE = TRUE | FALSE
     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     EMPTY_FIELD_AS_NULL = TRUE | FALSE
     SKIP_BYTE_ORDER_MARK = TRUE | FALSE
     ENCODING = '<string>' | UTF8
-- If TYPE = JSON
     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     DATE_FORMAT = '<string>' | AUTO
     TIME_FORMAT = '<string>' | AUTO
     TIMESTAMP_FORMAT = '<string>' | AUTO
     BINARY_FORMAT = HEX | BASE64 | UTF8
     TRIM_SPACE = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
     FILE_EXTENSION = '<string>'
     ENABLE_OCTAL = TRUE | FALSE
     ALLOW_DUPLICATE = TRUE | FALSE
     STRIP_OUTER_ARRAY = TRUE | FALSE
     STRIP_NULL_VALUES = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     IGNORE_UTF8_ERRORS = TRUE | FALSE
     SKIP_BYTE_ORDER_MARK = TRUE | FALSE
-- If TYPE = AVRO
     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     TRIM_SPACE = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
-- If TYPE = ORC
     TRIM_SPACE = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
-- If TYPE = PARQUET
     COMPRESSION = AUTO | LZO | SNAPPY | NONE
     SNAPPY_COMPRESSION = TRUE | FALSE
     BINARY_AS_TEXT = TRUE | FALSE
     USE_LOGICAL_TYPE = TRUE | FALSE
     TRIM_SPACE = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
-- If TYPE = XML
     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     IGNORE_UTF8_ERRORS = TRUE | FALSE
     PRESERVE_SPACE = TRUE | FALSE
     STRIP_OUTER_ELEMENT = TRUE | FALSE
     DISABLE_SNOWFLAKE_DATA = TRUE | FALSE
     DISABLE_AUTO_CONVERT = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     SKIP_BYTE_ORDER_MARK = TRUE | FALSE
  
  
--L9_CHALLENGE_FF

create or replace file format garden_plants.veggies.L9_CHALLENGE_FF
	type = 'CSV' -- error invalid value ['TSV'] for parameter 'type'
	field_delimiter = '\t' -- providing tab as delimiter
	skip_header = 1;
	
--The data in the file, with no FILE FORMAT specified
select $1
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv;

--Same file but with one of the file formats we created earlier  
select $1, $2, $3
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv
(file_format => garden_plants.veggies.COMMASEP_DBLQUOT_ONEHEADROW);


--passed

create or replace table LU_SOIL_TYPE(
SOIL_TYPE_ID number,	
SOIL_TYPE varchar(15),
SOIL_DESCRIPTION varchar(75)
 );

--copy into 

copy into LU_SOIL_TYPE
from @util_db.public.my_internal_stage
files = ( 'LU_SOIL_TYPE.tsv')
file_format = ( format_name=GARDEN_PLANTS.VEGGIES.L9_CHALLENGE_FF );



------------------------------------------------------------------------------------

🎯 Choose a File Format, write the COPY INTO, Load the File into the Table

Download this file: veg_plant_height.csv

Look at the data. Do not edit the data. Just look to understand it.  

Create a table called VEGETABLE_DETAILS_PLANT_HEIGHT in the VEGGIES schema. Use the header row of the file to get your column names. Choose good data types for each column. 

Upload the file into your stage.

Choose an existing file format (one you already created) that you think can be used to load the data.

Use a COPY INTO command to load the file from the Stage to the table you created. 

NOTE: The most common error is "Number of columns in file (1) does not match that of the corresponding table (4)" if you see this message, you have not chosen the correct file format. Or, sometimes, you are trying to load the wrong file. Double-check the file, file format, and table to make sure they match up. 

-------------------------------SNF

create or replace file format FORMAT_DETAIL_PLANT_HEIGHT
    type = 'CSV'
    field_delimiter = ','
    skip_header =1
    RECORD_DELIMITER = '\n' 
    FIELD_OPTIONALLY_ENCLOSED_BY = '\"';
	
select $1, $2, $3 , $4, $5 
from @util_db.public.my_internal_stage/veg_plant_height.csv;
	
select $1, $2, $3 , $4 ,$5
from @util_db.public.my_internal_stage/veg_plant_height.csv
(file_format => garden_plants.veggies.FORMAT_DETAIL_PLANT_HEIGHT);

	
copy into VEGETABLE_DETAILS_PLANT_HEIGHT 
from @util_db.public.my_internal_stage
files = ( 'veg_plant_height.csv')
file_format = ( format_name = FORMAT_DETAIL_PLANT_HEIGHT)
	
select * from VEGETABLE_DETAILS_PLANT_HEIGHT;

--------------------------------------------------------------------------------------

	
-----------------------------------------------------SNF	
 Create a Sequence 
A sequence is a counter. It can help you create unique ids for table rows. There are ways to create unique ids within a single table called an AUTO-INCREMENT column. Those are easy to set up and work well in a single table. A sequence can give you the power to split information across different tables and put the same ID in all tables as a way to make it easy to link them back together later. 


--Semi data STRUCTURE

Data type "VARIANT"

// JSON DDL Scripts
use database library_card_catalog;
use role sysadmin;

// Create an Ingestion Table for JSON Data
create table library_card_catalog.public.author_ingest_json
(
  raw_author variant
);



-- JSON DDL Scripts
use database library_card_catalog;
use role sysadmin;

-- Create an Ingestion Table for JSON Data
create table library_card_catalog.public.author_ingest_json
(
  raw_author variant
);

--Create File Format for JSON Data 
create or replace file format library_card_catalog.public.json_file_format
type = 'JSON' 
compression = 'AUTO' 
enable_octal = False
allow_duplicate = False
strip_outer_array = true -- this will result in data in different rows
strip_null_values = false
ignore_utf8_errors = false;

select $1
from @UTIL_DB.PUBLIC.MY_INTERNAL_STAGE/author_with_header.json;

select $1
from @UTIL_DB.PUBLIC.MY_INTERNAL_STAGE/author_with_header.json
(file_format => library_card_catalog.public.json_file_format) ;

----------------------------------

copy into LIBRARY_CARD_CATALOG.PUBLIC.AUTHOR_INGEST_JSON
from @util_db.public.my_internal_stage
files = ('author_with_header.json')
file_format = ( format_name = json_file_format);

select * from author_ingest_json;


---------------------------

--returns AUTHOR_UID value from top-level object's attribute
select raw_author:AUTHOR_UID
from author_ingest_json;

--returns the data in a way that makes it look like a normalized table
SELECT 
 raw_author:AUTHOR_UID
,raw_author:FIRST_NAME::STRING as FIRST_NAME
,raw_author:MIDDLE_NAME::STRING as MIDDLE_NAME
,raw_author:LAST_NAME::STRING as LAST_NAME
FROM AUTHOR_INGEST_JSON;


---------------------------

--Tweet data

//Create a new database to hold the Twitter file
create database SOCIAL_MEDIA_FLOODGATES 
comment = 'There\'s so much data from social media - flood warning';

use database SOCIAL_MEDIA_FLOODGATES;

//Create a table in the new database
create table SOCIAL_MEDIA_FLOODGATES.PUBLIC.TWEET_INGEST 
("RAW_STATUS" VARIANT) 
comment = 'Bring in tweets, one row per tweet or status entity';

//Create a JSON file format in the new database
create file format SOCIAL_MEDIA_FLOODGATES.PUBLIC.JSON_FILE_FORMAT 
type = 'JSON'
strip_outer_array = true;


--'@"UTIL_DB"."PUBLIC"."MY_INTERNAL_STAGE"/nutrition_tweets.json'


select $1
from @UTIL_DB.PUBLIC.MY_INTERNAL_STAGE/nutrition_tweets.json;

select $1
from '@"UTIL_DB"."PUBLIC"."MY_INTERNAL_STAGE"/nutrition_tweets.json'
(file_format => SOCIAL_MEDIA_FLOODGATES.PUBLIC.JSON_FILE_FORMAT ) ;

----------------------------------

copy into SOCIAL_MEDIA_FLOODGATES.PUBLIC.TWEET_INGEST 
from @util_db.public.my_internal_stage
files = ('nutrition_tweets.json')
file_format = ( format_name = JSON_FILE_FORMAT);

select * from TWEET_INGEST;

------------------------------------------------
//select statements as seen in the video
SELECT RAW_STATUS
FROM TWEET_INGEST;

SELECT RAW_STATUS:entities
FROM TWEET_INGEST;

SELECT RAW_STATUS:entities:hashtags
FROM TWEET_INGEST;

//Explore looking at specific hashtags by adding bracketed numbers
//This query returns just the first hashtag in each tweet
SELECT RAW_STATUS:entities:hashtags[0].text
FROM TWEET_INGEST;

//This version adds a WHERE clause to get rid of any tweet that 
//doesn't include any hashtags
SELECT RAW_STATUS:entities:hashtags[0].text
FROM TWEET_INGEST
WHERE RAW_STATUS:entities:hashtags[0].text is not null;

//Perform a simple CAST on the created_at key
//Add an ORDER BY clause to sort by the tweet's creation date
SELECT RAW_STATUS:created_at::DATE
FROM TWEET_INGEST
ORDER BY RAW_STATUS:created_at::DATE;

//Flatten statements that return the whole hashtag entity
SELECT value
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);

SELECT value
FROM TWEET_INGEST
,TABLE(FLATTEN(RAW_STATUS:entities:hashtags));

//Flatten statement that restricts the value to just the TEXT of the hashtag
SELECT value:text
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);


//Flatten and return just the hashtag text, CAST the text as VARCHAR
SELECT value:text::VARCHAR
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);

//Flatten and return just the hashtag text, CAST the text as VARCHAR
// Use the AS command to name the column
SELECT value:text::VARCHAR AS THE_HASHTAG
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);

//Add the Tweet ID and User ID to the returned table
SELECT RAW_STATUS:user:id AS USER_ID
,RAW_STATUS:id AS TWEET_ID
,value:text::VARCHAR AS HASHTAG_TEXT
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);


--------------------------------------------------------------------------------------

▪️ | 🎉 Badge Time! | ESS-DWW Courseware | Snowflake University: On-Demand

https://learn.snowflake.com/courses/course-v1:snowflake+ESS_DWW_101+2021/courseware/26086155ba0b43d28755a2d5a2111756/35515f95be0945e7b30859714e336237/?child=first

Home - Snowflake

https://app.snowflake.com/baqcvdr/lrb05308/#/homepage
