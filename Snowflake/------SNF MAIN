####################################################################################

Badge 1: Data Warehousing Workshop

####################################################################################

-------------------------------------------------------------SNF

Databases and schemas are used to organize data stored in Snowflake:

A database is a logical grouping of schemas. Each database belongs to a single Snowflake account.

A schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.

Databases are used to group datasets (tables) together. A second-level organizational grouping, within a database, is called a schema. Every time you create a database,
Snowflake will automatically create two schemas for you.

The INFORMATION_SCHEMA schema holds a collection of views.  The INFORMATION_SCHEMA schema cannot be deleted (dropped), renamed, or moved.

The PUBLIC schema is created empty and you can fill it with tables, views and other things over time. The PUBLIC schema can be dropped, renamed, or moved at any time.


Identity vs ACCESS
Username password one time Vs role base continuous
Authentication vs Authorization
 
Roles
--5 Roles
ACCOUNTADMIN  - Master of ACCOUNT
SYSADMIN	  - Master of creation of DB and schemas

Role-based Access Control(RBAC)
Discretionary Access Control (DAC). -you own what you create
SNF uses both RABC and DAC


--ROLE CREATION AS INHERITANCE
ACCOUNTADMIN --parent
SECURITYADMIN and SYSADMIN --child

ACCOUNTADMIN -it can manage all aspect of account
	SECURITYADMIN -security
	SYSADMIN -it can manage and create database and warehouse
	USERADMIN -it can manage users


-------------------------------------------------------------------------SNF

IN SNF WAREHOUSE IS NOT PLACE TO STORE DATA , IT IS COMPUTE RESOURCE, OR COMPUTING POWER. WAREHOUSE IS THE MACHINE THAT CRUCHES DATA , BIG MACHINE CAN CRUCH THAT REALLY
FAST LIKE WH-XL

============================================================================

Data warehouses vs Databases

Data warehouses
Store data from multiple sources in a fixed schema for long-term storage and reporting. They are useful for analytics, reporting, and read-heavy operations. 
Data warehouses can include current and historical data, and they allow business analysts and data scientists to analyze the data. 
They can also help companies make analytical queries to track variables for business intelligence. Data warehouses are well-suited for OLAP solutions, 
which can aggregate data for large-scale analytics. 

Databases
Store structured data in tables for short-term data storage and data manipulation tasks. They are ideal for transactional data and applications that require frequent 
read/write operations. Databases are mainly used for data manipulation and retrieval tasks, and they serve as a management system to organize and process data. 
They are also good for OLTP solutions, which provide fast data access

--SNOWFLAKE AND ITS BASIC SERVICE -DATA WAREHOUSING

Inside Snowflake, the virtual warehouse is a cluster of compute resources. It provides resources ‚Äî including memory, temporary storage and CPU ‚Äî to perform tasks such as
DML operation and SQL execution.


--WHAT IS SNOWFLAKE?


Snowflake is a cloud-based data platform that offers data warehousing as its core service. Every Snowflake customer gains access to a dedicated virtual warehouse, which
they build based on their storage and processing needs. After that, they migrate their data to the warehouse and implement a new data architecture, which results in all 
data pipelines leading to the central data repository.

Some of the features of the Snowflake data warehouse include:

Scalability ‚Äì 
	Snowflake uses massively parallel processing (MPP) architecture, which distributes data across a cluster of independently running machines. This allows the 
warehouse to scale as needed, multiple times a day. When you have multiple users batch processing or stream processing large volumes of data simultaneously, the platform 
scales out and dedicates additional resources to you. It scales back down automatically afterward.
	
Built-in security features ‚Äì 
	There are several security measures built into the platform, such as multi-factor authentication for all users, end-to-end encryption of data, and IP whitelisting.
	
Multi-cloud deployment ‚Äì 
	The warehouse can be deployed on AWSopens in new tab, Azureopens in new tab, and Google Cloudopens in new tab.
--Grant ownership for all

--After cloning the database, transfer ownership to another role using the GRANT OWNERSHIP (see also example) function using COPY CURRENT GRANTS clause, for example

GRANT OWNERSHIP ON DATABASE mydb TO ROLE developer COPY CURRENT GRANTS;  
GRANT OWNERSHIP ON ALL SCHEMAS IN DATABASE mydb TO ROLE developer COPY CURRENT GRANTS;
GRANT OWNERSHIP ON ALL TABLES IN DATABASE mydb TO ROLE developer COPY CURRENT GRANTS; 
GRANT OWNERSHIP ON ALL VIEWS IN DATABASE mydb TO ROLE developer COPY CURRENT GRANTS;

create DATABASE GARDEN_PLANTS;

drop schema GARDEN_PLANTS.PUBLIC;

--VEGGIES, FRUITS and FLOWERS

create schema GARDEN_PLANTS.VEGGIES;
create schema GARDEN_PLANTS.FRUITS;
create schema GARDEN_PLANTS.FLOWERS;

describe database GARDEN_PLANTS;

show databases;
show schemas;

SHOW SCHEMAS IN ACCOUNT;


------------------------------------------------SNF
	
DATA MODELING and NORMALIZATION

--DATA MODELING

Data modeling is the process of organizing and mapping data using simplified diagrams, symbols, and text to represent data associations and flow.

Types of Approaches
There are four primary approaches to data modeling.  

1. Hierarchical
	A hierarchical database model organizes data into tree-like structures with data stored as interconnected records with one-to-many arrangements. Hierarchical
	database models are standard in XML and GIS.  

2. Relational
	A relational data model, AKA a relational model, manages data by providing methodology for specifying data and queries. Most relational data models use SQL for 
	data definition and query language.

3. Entity-relationship
	Entity-relationship models use diagrams to portray data and their relationships. Integrated with relational data models, entity-relationship models graphically 
	depict data elements to understand underlying models. 

4. Graph
	Graph data models are visualizations of complex relationships within data sets that are limited by a chosen domain.


Types of Data Models
There are three primary types of data models. 

1. Conceptual, defining what data system contains, used to organize, scope, and define business concepts and rules.

2. Logical, defining how a data system should be implemented, used to develop a technical map of rules and data structures.

3. Physical, defining how the data system will be implemented according to the specific use case.


--WHAT IS DATA NORMALIZATION?

Data normalization applies a set of formal rules to develop standardized, organized data, and eliminates data anomalies that cause difficulty for analysis. The clean data 
can then be easily grouped, understood, and interpreted. Without normalization, valuable data will go unused. 

Depending on your use case, data normalization may happen prior to or after loading the data into your data warehouse or platform. Some platforms, such as Snowflake, allow 
complete flexibility so you can store massive amounts of raw data and normalize only the data you need as you need it.

--DATA NORMALIZATION RULES
Data normalization rules are sequential‚Äîas you move through each rule, you normalize the data further. For this reason, you can think of normalization rules as ‚Äúlevels‚Äù of
normalization. Although there are five rules in total, only three are commonly used for practical reasons, since too much normalization results in inflexibility in the 
data model. 

--1NF (First Normal Form) rule (Remove duplicate)
	The first rule is about ensuring there are no repeating entries in a group. All attributes must have a unique name, and entities may consist of only two dimensions.
(You‚Äôll need to create a new entity for additional dimensions.) Each entry must have only a single value for each cell, and each record must be unique. The goal of this 
first rule is to make it easier to search the data.

--2NF (Second Normal Form) rule (Unique primary key by segregating table further)
	The second rule is designed to eliminate redundant data. Before applying the 2NF rule, you must be sure that the 1NF rule has been fully applied. Data that is in 
the 2NF state will have only one primary key. For this reason, you must separate data by placing all subsets of data that can be placed in multiple rows into separate 
tables. Relationships can then be created via foreign key labels.

--3NF (Third Normal Form) rule (remove transitive)
	The third rule eliminates transitive dependency. As before, data must have achieved 1NF and 2NF status before you can apply the 3NF rule. Transitive dependency is
when a nonprime attribute depends on other nonprime attributes instead of depending on the prime attributes or primary key. So the third rule ensures that no attribute 
within an entity is dependent on a nonprime attribute that depends on the primary key. For this reason, if the primary key is changed, all data impacted by the change has 
to be put into a new table with a new foreign key.

------------------------------------------------------------------SNF
GETTING ROWS OF DATA INTO TABLE

Using an INSERT statement from the Worksheet. 
Using the Load Data Wizard.
Using COPY INTO statements. 

------------------------------------------------------------------SNF
--If Warehouse in SNF is NOT place to store data then
--WHERE DOES DATA IS STORED IN SNOWFLAKE?


Database Storage
When data is loaded into Snowflake, Snowflake reorganizes that data into its internal optimized, compressed, columnar format. Snowflake stores this optimized data in cloud
storage.

--Elaborate

Snowflake, a cloud-based data warehousing platform, stores data in cloud storage after reorganizing it into a compressed, COLUMNAR FORMAT. Snowflake manages all aspects of 
data storage, including organization, file size, structure, compression, metadata, and statistics.
 
Snowflake storage layer uses scalable CLOUD BLOB STORAGE to store data, tables, and query results. The storage layer is designed to scale independently from compute 
resources, and customers can increase and reduce storage and analytics requirements independently. 

Snowflake stores data in different places depending on the version of Snowflake: 
AWS version: Stores data on S3 
Azure version: Stores data on Azure Blob 

Snowflake stores all data in database tables, which are logically structured as collections of columns and rows. Within each micro-partition, data is stored in a columnar 
data structure, allowing better compression and efficient access only to those columns required by a query. 
Customers can only access the data objects stored by Snowflake through SQL query operations run using Snowflake. 

-------------------------------------------------------SNF

--DEFINING "WAREHOUSE" IN SNOWFLAKE:

People who have been working with data for awhile might think of the term "Data Warehouse" as referring to a special collection of data structures, but in Snowflake, warehouses do not store data.
In Snowflake, Warehouses are "workforces" -- they are used to perform the processing of data. 

Warehouse in SNF is COMPUTING RESOURCE 

When you create a Warehouse in Snowflake, you are defining a "workforce."

Teams are CLUSTERS, Team Members are SERVERS: 

The workforce of each warehouse is a team. A small warehouse has a small team, but just one team. An extra-large warehouse has a large team, but just one team.  
Snowflake Warehouse Sizes like eXtra-Small, Small, Medium, etc. all have one cluster. A small warehouse has one cluster made up of just a few servers. A larger warehouse has one cluster, made up of more servers.
Scaling Up and Down: 
Changing the size of warehouse changes the number of servers in the cluster. 
Changing the size of an existing warehouse is called scaling up or scaling down.

Scaling In and Out: 

If multi-cluster/elastic warehousing is available (Enterprise edition or above) a warehouse is capable of scaling out in times of increased demand.  (Adding temporary teams, made up of a collection of temporary workers). 
If multi-cluster scaling out takes place, clusters are added for the period of demand and then clusters are removed (snap back) when demand decreases. (Removing temporary teams). 
The number of servers in the original cluster dictates the number of servers in each cluster during periods where the warehouse scales out by adding clusters. 


--------------------------------------------------------SNF

when an xs warehouse automatically adds cluster to handle an increase workload, this  is example of scaling OUT.
Opposite of scaling out is snapping back.

Cluster is the group of servers.

All warehouse have single cluster until scaling out.

Number of server is only thing that changes.

--------------------------------------------------------snf
--api_integration

use role accountadmin;

create or replace api integration dora_api_integration
api_provider = aws_api_gateway
api_aws_role_arn = 'arn:aws:iam::321463406630:role/snowflakeLearnerAssumedRole'
enabled = true
api_allowed_prefixes = ('https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora');


--create grader function 
use role accountadmin;  

create or replace external function util_db.public.grader(
      step varchar
    , passed boolean
    , actual integer
    , expected integer
    , description varchar)
returns variant
api_integration = dora_api_integration 
context_headers = (current_timestamp, current_account, current_statement, current_account_name) 
as 'https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora/grader'

--------------------------------------------------------------------------
----------------------------------------snf
To COPY INTO statement, it is best to have 4 things in place:

A table 
A stage object
A file
A file format 
The file format is sort of optional, but it is a cleaner process if you have one, and we do!

copy into my_table_name
from @my_internal_stage
files = ( 'IF_I_HAD_A_FILE_LIKE_THIS.txt')
file_format = ( format_name='EXAMPLE_FILEFORMAT' );

-------------

HOW TO CREATE STAGE AND FILE FORMAT

--create table

create or replace table vegetable_details_soil_type
( plant_name varchar(25)
 ,soil_type number(1,0)
);

--create format 

create file format garden_plants.veggies.PIPECOLSEP_ONEHEADROW 
    type = 'CSV'--csv is used for any flat file (tsv, pipe-separated, etc)
    field_delimiter = '|' --pipes as column separators
    skip_header = 1 --one header row to skip
	
-- copy into

copy into vegetable_details_soil_type
from @util_db.public.my_internal_stage
files = ( 'VEG_NAME_TO_SOIL_TYPE_PIPE.txt')
file_format = ( format_name=GARDEN_PLANTS.VEGGIES.PIPECOLSEP_ONEHEADROW );


----------------------------------------------

--The data in the file, with no FILE FORMAT specified
select $1
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv;

--Same file but with one of the file formats we created earlier  
select $1, $2, $3
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv
(file_format => garden_plants.veggies.COMMASEP_DBLQUOT_ONEHEADROW);

--Same file but with the other file format we created earlier
select $1, $2, $3
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv
(file_format => garden_plants.veggies.PIPECOLSEP_ONEHEADROW );

---------------------------------------------

--CREATE FILE FORMAT
Creates a named file format that describes a set of staged data to access or load into Snowflake tables.
--query syntax
CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY | VOLATILE } ] FILE FORMAT [ IF NOT EXISTS ] <name>
  [ TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM} [ formatTypeOptions ] ]
  [ COMMENT = '<string_literal>' ]
  
 formatTypeOptions ::=
-- If TYPE = CSV
     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     RECORD_DELIMITER = '<character>' | NONE
     FIELD_DELIMITER = '<character>' | NONE
     FILE_EXTENSION = '<string>'
     PARSE_HEADER = TRUE | FALSE
     SKIP_HEADER = <integer>
     SKIP_BLANK_LINES = TRUE | FALSE
     DATE_FORMAT = '<string>' | AUTO
     TIME_FORMAT = '<string>' | AUTO
     TIMESTAMP_FORMAT = '<string>' | AUTO
     BINARY_FORMAT = HEX | BASE64 | UTF8
     ESCAPE = '<character>' | NONE
     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE
     TRIM_SPACE = TRUE | FALSE
     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     EMPTY_FIELD_AS_NULL = TRUE | FALSE
     SKIP_BYTE_ORDER_MARK = TRUE | FALSE
     ENCODING = '<string>' | UTF8
-- If TYPE = JSON
     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     DATE_FORMAT = '<string>' | AUTO
     TIME_FORMAT = '<string>' | AUTO
     TIMESTAMP_FORMAT = '<string>' | AUTO
     BINARY_FORMAT = HEX | BASE64 | UTF8
     TRIM_SPACE = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
     FILE_EXTENSION = '<string>'
     ENABLE_OCTAL = TRUE | FALSE
     ALLOW_DUPLICATE = TRUE | FALSE
     STRIP_OUTER_ARRAY = TRUE | FALSE
     STRIP_NULL_VALUES = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     IGNORE_UTF8_ERRORS = TRUE | FALSE
     SKIP_BYTE_ORDER_MARK = TRUE | FALSE
-- If TYPE = AVRO
     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     TRIM_SPACE = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
-- If TYPE = ORC
     TRIM_SPACE = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
-- If TYPE = PARQUET
     COMPRESSION = AUTO | LZO | SNAPPY | NONE
     SNAPPY_COMPRESSION = TRUE | FALSE
     BINARY_AS_TEXT = TRUE | FALSE
     USE_LOGICAL_TYPE = TRUE | FALSE
     TRIM_SPACE = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     NULL_IF = ( '<string>' [ , '<string>' ... ] )
-- If TYPE = XML
     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE
     IGNORE_UTF8_ERRORS = TRUE | FALSE
     PRESERVE_SPACE = TRUE | FALSE
     STRIP_OUTER_ELEMENT = TRUE | FALSE
     DISABLE_SNOWFLAKE_DATA = TRUE | FALSE
     DISABLE_AUTO_CONVERT = TRUE | FALSE
     REPLACE_INVALID_CHARACTERS = TRUE | FALSE
     SKIP_BYTE_ORDER_MARK = TRUE | FALSE
  
  
--L9_CHALLENGE_FF

create or replace file format garden_plants.veggies.L9_CHALLENGE_FF
	type = 'CSV' -- error invalid value ['TSV'] for parameter 'type'
	field_delimiter = '\t' -- providing tab as delimiter
	skip_header = 1;
	
--The data in the file, with no FILE FORMAT specified
select $1
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv;

--Same file but with one of the file formats we created earlier  
select $1, $2, $3
from @util_db.public.my_internal_stage/LU_SOIL_TYPE.tsv
(file_format => garden_plants.veggies.COMMASEP_DBLQUOT_ONEHEADROW);


--passed

create or replace table LU_SOIL_TYPE(
SOIL_TYPE_ID number,	
SOIL_TYPE varchar(15),
SOIL_DESCRIPTION varchar(75)
 );

--copy into 

copy into LU_SOIL_TYPE
from @util_db.public.my_internal_stage
files = ( 'LU_SOIL_TYPE.tsv')
file_format = ( format_name=GARDEN_PLANTS.VEGGIES.L9_CHALLENGE_FF );



------------------------------------------------------------------------------------

üéØ Choose a File Format, write the COPY INTO, Load the File into the Table

Download this file: veg_plant_height.csv

Look at the data. Do not edit the data. Just look to understand it.  

Create a table called VEGETABLE_DETAILS_PLANT_HEIGHT in the VEGGIES schema. Use the header row of the file to get your column names. Choose good data types for each column. 

Upload the file into your stage.

Choose an existing file format (one you already created) that you think can be used to load the data.

Use a COPY INTO command to load the file from the Stage to the table you created. 

NOTE: The most common error is "Number of columns in file (1) does not match that of the corresponding table (4)" if you see this message, you have not chosen the correct file format. Or, sometimes, you are trying to load the wrong file. Double-check the file, file format, and table to make sure they match up. 

-------------------------------SNF

create or replace file format FORMAT_DETAIL_PLANT_HEIGHT
    type = 'CSV'
    field_delimiter = ','
    skip_header =1
    RECORD_DELIMITER = '\n' 
    FIELD_OPTIONALLY_ENCLOSED_BY = '\"';
	
select $1, $2, $3 , $4, $5 
from @util_db.public.my_internal_stage/veg_plant_height.csv;
	
select $1, $2, $3 , $4 ,$5
from @util_db.public.my_internal_stage/veg_plant_height.csv
(file_format => garden_plants.veggies.FORMAT_DETAIL_PLANT_HEIGHT);

	
copy into VEGETABLE_DETAILS_PLANT_HEIGHT 
from @util_db.public.my_internal_stage
files = ( 'veg_plant_height.csv')
file_format = ( format_name = FORMAT_DETAIL_PLANT_HEIGHT)
	
select * from VEGETABLE_DETAILS_PLANT_HEIGHT;

--------------------------------------------------------------------------------------

	
-----------------------------------------------------SNF	
 Create a Sequence 
A sequence is a counter. It can help you create unique ids for table rows. There are ways to create unique ids within a single table called an AUTO-INCREMENT column. Those are easy to set up and work well in a single table. A sequence can give you the power to split information across different tables and put the same ID in all tables as a way to make it easy to link them back together later. 


--Semi data STRUCTURE

Data type "VARIANT"

// JSON DDL Scripts
use database library_card_catalog;
use role sysadmin;

// Create an Ingestion Table for JSON Data
create table library_card_catalog.public.author_ingest_json
(
  raw_author variant
);



-- JSON DDL Scripts
use database library_card_catalog;
use role sysadmin;

-- Create an Ingestion Table for JSON Data
create table library_card_catalog.public.author_ingest_json
(
  raw_author variant
);

--Create File Format for JSON Data 
create or replace file format library_card_catalog.public.json_file_format
type = 'JSON' 
compression = 'AUTO' 
enable_octal = False
allow_duplicate = False
strip_outer_array = true -- this will result in data in different rows
strip_null_values = false
ignore_utf8_errors = false;

select $1
from @UTIL_DB.PUBLIC.MY_INTERNAL_STAGE/author_with_header.json;

select $1
from @UTIL_DB.PUBLIC.MY_INTERNAL_STAGE/author_with_header.json
(file_format => library_card_catalog.public.json_file_format) ;

----------------------------------

copy into LIBRARY_CARD_CATALOG.PUBLIC.AUTHOR_INGEST_JSON
from @util_db.public.my_internal_stage
files = ('author_with_header.json')
file_format = ( format_name = json_file_format);

select * from author_ingest_json;


---------------------------

--returns AUTHOR_UID value from top-level object's attribute
select raw_author:AUTHOR_UID
from author_ingest_json;

--returns the data in a way that makes it look like a normalized table
SELECT 
 raw_author:AUTHOR_UID
,raw_author:FIRST_NAME::STRING as FIRST_NAME
,raw_author:MIDDLE_NAME::STRING as MIDDLE_NAME
,raw_author:LAST_NAME::STRING as LAST_NAME
FROM AUTHOR_INGEST_JSON;


---------------------------

--Tweet data

//Create a new database to hold the Twitter file
create database SOCIAL_MEDIA_FLOODGATES 
comment = 'There\'s so much data from social media - flood warning';

use database SOCIAL_MEDIA_FLOODGATES;

//Create a table in the new database
create table SOCIAL_MEDIA_FLOODGATES.PUBLIC.TWEET_INGEST 
("RAW_STATUS" VARIANT) 
comment = 'Bring in tweets, one row per tweet or status entity';

//Create a JSON file format in the new database
create file format SOCIAL_MEDIA_FLOODGATES.PUBLIC.JSON_FILE_FORMAT 
type = 'JSON'
strip_outer_array = true;


--'@"UTIL_DB"."PUBLIC"."MY_INTERNAL_STAGE"/nutrition_tweets.json'


select $1
from @UTIL_DB.PUBLIC.MY_INTERNAL_STAGE/nutrition_tweets.json;

select $1
from '@"UTIL_DB"."PUBLIC"."MY_INTERNAL_STAGE"/nutrition_tweets.json'
(file_format => SOCIAL_MEDIA_FLOODGATES.PUBLIC.JSON_FILE_FORMAT ) ;

----------------------------------

copy into SOCIAL_MEDIA_FLOODGATES.PUBLIC.TWEET_INGEST 
from @util_db.public.my_internal_stage
files = ('nutrition_tweets.json')
file_format = ( format_name = JSON_FILE_FORMAT);

select * from TWEET_INGEST;

------------------------------------------------
//select statements as seen in the video
SELECT RAW_STATUS
FROM TWEET_INGEST;

SELECT RAW_STATUS:entities
FROM TWEET_INGEST;

SELECT RAW_STATUS:entities:hashtags
FROM TWEET_INGEST;

//Explore looking at specific hashtags by adding bracketed numbers
//This query returns just the first hashtag in each tweet
SELECT RAW_STATUS:entities:hashtags[0].text
FROM TWEET_INGEST;

//This version adds a WHERE clause to get rid of any tweet that 
//doesn't include any hashtags
SELECT RAW_STATUS:entities:hashtags[0].text
FROM TWEET_INGEST
WHERE RAW_STATUS:entities:hashtags[0].text is not null;

//Perform a simple CAST on the created_at key
//Add an ORDER BY clause to sort by the tweet's creation date
SELECT RAW_STATUS:created_at::DATE
FROM TWEET_INGEST
ORDER BY RAW_STATUS:created_at::DATE;

//Flatten statements that return the whole hashtag entity
SELECT value
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);

SELECT value
FROM TWEET_INGEST
,TABLE(FLATTEN(RAW_STATUS:entities:hashtags));

//Flatten statement that restricts the value to just the TEXT of the hashtag
SELECT value:text
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);


//Flatten and return just the hashtag text, CAST the text as VARCHAR
SELECT value:text::VARCHAR
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);

//Flatten and return just the hashtag text, CAST the text as VARCHAR
// Use the AS command to name the column
SELECT value:text::VARCHAR AS THE_HASHTAG
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);

//Add the Tweet ID and User ID to the returned table
SELECT RAW_STATUS:user:id AS USER_ID
,RAW_STATUS:id AS TWEET_ID
,value:text::VARCHAR AS HASHTAG_TEXT
FROM TWEET_INGEST
,LATERAL FLATTEN
(input => RAW_STATUS:entities:hashtags);


--------------------------------------------------------------------------------------

‚ñ™Ô∏è | üéâ Badge Time! | ESS-DWW Courseware | Snowflake University: On-Demand

https://learn.snowflake.com/courses/course-v1:snowflake+ESS_DWW_101+2021/courseware/26086155ba0b43d28755a2d5a2111756/35515f95be0945e7b30859714e336237/?child=first

Home - Snowflake

https://app.snowflake.com/baqcvdr/lrb05308/#/homepage

BADGE

https://achieve.snowflake.com/730d2107-b912-484c-8751-48dd830e2b1e#gs.e373mb

Courses

https://learn.snowflake.com/en/courses/


=========================================================================================================================================================================

Badge 2- 


--Joining local data with shared data
create or replace table intl_db.public.INT_STDS_ORG_3166 
(iso_country_name varchar(100), 
 country_name_official varchar(200), 
 sovreignty varchar(40), 
 alpha_code_2digit varchar(2), 
 alpha_code_3digit varchar(3), 
 numeric_country_code integer,
 iso_subdivision varchar(15), 
 internet_domain_code varchar(10)
);


create or replace file format util_db.public.PIPE_DBLQUOTE_HEADER_CR 
  type = 'CSV' --use CSV for any flat file
  compression = 'AUTO' 
  field_delimiter = '|' --pipe or vertical bar
  record_delimiter = '\r' --carriage return
  skip_header = 1  --1 header row
  field_optionally_enclosed_by = '\042'  --double quotes
  trim_space = FALSE;

--Data files for this course are available from an s3 bucket named uni-cmcw.  There is only one s3 bucket in the whole world with that name and it belongs to this course.
show stages in account;

create or replace stage util_db.public.aws_s3_bucket url = 's3://uni-cmcw';
--creating stage

--The file you will be loading is called iso_countries_utf8_pipe.csv. BUT remember that AWS is very case sensitive, so be sure to look up the EXACT spelling of the file name for your COPY INTO statement. Remember that you can view the files in the stage either by navigating to the stage and enabling the directory table, or by running a list command like this

list @util_db.public.aws_s3_bucket;

--copy inot from S3 bucket
copy into intl_db.public.INT_STDS_ORG_3166 
from @UTIL_DB.PUBLIC.AWS_S3_BUCKET
files = ('ISO_Countries_UTF8_pipe.csv')
file_format = (format_name = 'PIPE_DBLQUOTE_HEADER_CR');

select count(*) as found, '249' as expected 
from INTL_DB.PUBLIC.INT_STDS_ORG_3166; 

--How to Test Whether You Set Up Your Table in the Right Place with the Right Name
 
select count(*) as OBJECTS_FOUND
from <database name>.INFORMATION_SCHEMA.TABLES 
where table_schema=<schema name> 
and table_name= <table name>;


--------------------------------------------------------------------------------------------snf

-- So if we are looking for INTL_DB.PUBLIC.INT_STDS_ORG_3166 we can run this command to check: 

select count(*) as OBJECTS_FOUND
from INTL_DB.INFORMATION_SCHEMA.TABLES 
where table_schema='PUBLIC'
and table_name= 'INT_STDS_ORG_3166';

--We can "ask" the Information Schema Table called "Tables" if our table has the expected number of rows with a command like this:

select row_count
from <database name>.INFORMATION_SCHEMA.TABLES 
where table_schema=<schema name> 
and table_name= <table name>;

--for  INTL_DB.PUBLIC.INT_STDS_ORG_3166

select row_count
from INTL_DB.INFORMATION_SCHEMA.TABLES 
where table_schema='PUBLIC'
and table_name= 'INT_STDS_ORG_3166';

--use of views is to capture a complex select statement and make it simple to run 

----------------------------------------------------------------------------------------------
--Create Table Currencies

create table intl_db.public.CURRENCIES 
(
  currency_ID integer, 
  currency_char_code varchar(3), 
  currency_symbol varchar(4), 
  currency_digital_code varchar(3), 
  currency_digital_name varchar(30)
)
  comment = 'Information about currencies including character codes, symbols, digital codes, etc.';

  create table intl_db.public.COUNTRY_CODE_TO_CURRENCY_CODE 
  (
    country_char_code varchar(3), 
    country_numeric_code integer, 
    country_name varchar(100), 
    currency_name varchar(100), 
    currency_char_code varchar(3), 
    currency_numeric_code integer
  ) 
  comment = 'Mapping table currencies to countries';

   create file format util_db.public.CSV_COMMA_LF_HEADER
  type = 'CSV' 
  field_delimiter = ',' 
  record_delimiter = '\n' -- the n represents a Line Feed character
  skip_header = 1 
;

--viweing aws stage file data with file format CSV_COMMA_LF_HEADER

select $1, $2 ,$3, $4, $5
from '@"UTIL_DB"."PUBLIC"."AWS_S3_BUCKET"/currencies.csv'
(file_format => UTIL_DB.PUBLIC.CSV_COMMA_LF_HEADER);

--load data from stage to table
copy into INTL_DB.PUBLIC.CURRENCIES
from @UTIL_DB.PUBLIC.AWS_S3_BUCKET
files = ('currencies.csv')
file_format = (format_name = 'UTIL_DB.PUBLIC.CSV_COMMA_LF_HEADER') ;


--from @STAGE_NAME

select $1, $2 ,$3, $4, $5, $6
from '@"UTIL_DB"."PUBLIC"."AWS_S3_BUCKET"/country_code_to_currency_code.csv'
(file_format => UTIL_DB.PUBLIC.CSV_COMMA_LF_HEADER);

copy into INTL_DB.PUBLIC.COUNTRY_CODE_TO_CURRENCY_CODE
from @UTIL_DB.PUBLIC.AWS_S3_BUCKET
files = ('country_code_to_currency_code.csv')
file_format = (format_name = 'UTIL_DB.PUBLIC.CSV_COMMA_LF_HEADER');

---

select * from INTL_DB.PUBLIC.CURRENCIES;

------------------------------------------------------------------------------

-----------------------------------------SNF
--two SNOWFALKE accounts

1-
https://app.snowflake.com/nykiqhf/acme/#/homepage
ACME
ACME_ADMIN
Yay123456
NYKIQHF.ACME

2-
https://app.snowflake.com/nykiqhf/qrb06978/#/homepage
WORLD_EMP_DATA (WDE)
Jai1097


--Sharing data between two account 

--adding in Data product -> Provider Sharing and ACME as the consumer

--so whatever data WORLD_EMP_DATA have can be shared to ACME_ADMIN (in private sharing)

--How Sharing Will Work
--Because your trial account (WDE) and your ACME account are on different clouds, in different regions, Snowflake will automatically perform replication to the other cloud/region.

--This will be a cost that WDE/Osiris covers. If Osiris doesn't want to cover that cost, he could insist that Lottie's team get a Snowflake account on the same cloud and in the same region as his primary account. This may become part of their negotiations.

--If we look at the WDE Account (your original trial account) we see that a new database with an odd name has been created. 

--üìì What is a Listing?
--A listing is a vehicle for sharing. As we've seen, we were able to create a data share named WDE_INTL_CURRENCIES and enclose it in a listing named "International Currencies."


--SNOWFLAKE PRICING

--The Snowflake architecture separates data warehousing into three distinct layers: storage, virtual warehouses (compute), and cloud services. Snowflake pricing is based on the actual usage of these layers and of serverless features Snowpipe, replication and Clustering.

--Storage
--A monthly fee for data stored in Snowflake is calculated using the average amount of storage used per month, after compression, for data ingested into Snowflake. Depending on file types, compression can reduce the total storage needs substantially.

--Virtual Warehouses (Compute)
--A virtual warehouse is one or more compute clusters that enable customers to load data and perform queries. Customers pay for virtual warehouses using Snowflake credits.

--Cloud Services
--The cloud services layer provides all permanent state management and overall coordination of Snowflake. Customers pay for cloud services using Snowflake credits.



--------------------------------------------------SNF
Badge 2: Collaboration, Marketplace & Cost Estimation Workshop
A course covering Snowflake iss revolutionary collaboration technologies. Learn to create listings, get listings sent by other accounts, and shop for data sets on the Snowflake Marketplace. Understand how your organization can get rid of nightly extracts and cumbersome ETL processes. This course also helps those new to SQL brush up on some important SQL skills including basics like DISTINCT, GROUP BY, ORDER BY, JOINS, and control-of-flow like BEGIN, END, DECLARE and FOR. Finally, the course covers cost categories, cost estimation, and some beginner-level cost control mechanisms.



====================================================================

BELOW IS THE EXMAPLE OF SNF USE FROM START FROM ACCOUNT CREATION -> WAREHOUSE, RESOURCE MONITOR CREATION ->DATABASE->SCHEMA -> TABLE CREATION ->FILE FORMAT->CREATE AN EXTERNAL STAGE -> COPY INTO TABLES

3-GCP
https://nykiqhf-auto_data_unlimited.snowflakecomputing.com
AUTO_DATA_UNLIMITED
ADU_ADMIN
YaY123456

create database VIN;

create schema VIN.DECODE;

drop schema VIN.PUBLIC;

--We need a table that will allow WMIs to be decoded into Manufacturer Name, Country and Vehicle Type
CREATE TABLE vin.decode.wmi_to_manuf 
(
     wmi	    varchar(6)
    ,manuf_id	    number(6)
    ,manuf_name	    varchar(50)
    ,country	    varchar(50)
    ,vehicle_type    varchar(50)
 );
 
--We need a table that will allow you to go from Manufacturer to Make
--For example, Mercedes AG of Germany and Mercedes USA both roll up into Mercedes
--But they use different WMI Codes
CREATE TABLE vin.decode.manuf_to_make
(
     manuf_id	number(6)
    ,make_name	varchar(50)
    ,make_id	number(5)
);

--We need a table that can decode the model year
-- The year 2001 is represented by the digit 1
-- The year 2020 is represented by the letter L
CREATE TABLE vin.decode.model_year
(
     model_year_code	varchar(1)
    ,model_year_name	varchar(4)
);

--We need a table that can decode which plant at which 
--the vehicle was assembled
--You might have code "A" for Honda and code "A" for Ford
--so you need both the Make and the Plant Code to properly decode 
--the plant code
CREATE TABLE vin.decode.manuf_plants
(
     make_id	number(5)
    ,plant_code	varchar(1)
    ,plant_name	varchar(75)
 );
 
--We need to use a combination of both the Make and VDS 
--to decode many attributes including the engine, transmission, etc
CREATE TABLE vin.decode.make_model_vds
(
     make_id	  number(3)
    ,model_id	  number(6)
    ,model_name	  varchar(50)
    ,vds	  varchar(5)
    ,desc1	  varchar(25)
    ,desc2	  varchar(25)
    ,desc3	  varchar(50)
    ,desc4	  varchar(25)
    ,desc5	  varchar(25)
    ,body_style	  varchar(25)
    ,engine	  varchar(100)
    ,drive_type	  varchar(50)
    ,transmission varchar(50)
    ,mpg  	varchar(25)
);


--Create a file format and then load each of the 5 Lookup Tables
--You need a file format if you want to load the table
CREATE FILE FORMAT vin.decode.comma_sep_oneheadrow 
type = 'CSV' 
field_delimiter = ',' 
record_delimiter = '\n' 
skip_header = 1 
field_optionally_enclosed_by = '"'  
trim_space = TRUE;


-- Even though our ADU Account is on GCP, we can still pull files from an AWS Stage. Storage from all 3 cloud providers work seamlessly with Snowflake accounts on any other provider.

-- It should be an External AWS Stage
-- The URL is  s3://uni-cmcw/  
-- The stage should be in the VIN.DECODE schema. 
-- You can name it whatever you want, but aws_s3_bucket might be easiest.
-- It should be owned by SYSADMIN


--approach via coding


-- External stages
-- In addition to loading directly from files in S3 buckets, Snowflake supports creating named external stages, which encapsulate all of the required information for staging files, including:

-- The S3 bucket where the files are staged.

-- The named storage integration object or S3 credentials for the bucket (if it is protected).

-- An encryption key (if the files in the bucket have been encrypted).


-- Create an external stage using SQL¬∂
-- Use the CREATE STAGE command to create an external stage using SQL.

-- The following example uses SQL to create an external stage named my_s3_stage that references a private/protected S3 bucket named mybucket with a folder path named encrypted_files/. The CREATE statement includes the s3_int storage integration that was created in Option 1: Configuring a Snowflake storage integration to access Amazon S3 to access the S3 bucket. The stage references a named file format object named my_csv_format, which describes the data in the files stored in the bucket path:

-- CREATE STAGE my_s3_stage
--   STORAGE_INTEGRATION = s3_int
--   URL = 's3://mybucket/encrypted_files/'
--   FILE_FORMAT = my_csv_format



COPY INTO vin.decode.wmi_to_manuf
from @vin.decode.aws_s3_bucket
files = ('Maxs_WMIToManuf_data.csv')
file_format =(format_name = vin.decode.comma_sep_oneheadrow);

COPY INTO vin.decode.manuf_to_make
from @vin.decode.aws_s3_bucket
files = ('Maxs_ManufToMake_Data.csv')
file_format =(format_name = vin.decode.comma_sep_oneheadrow);


COPY INTO vin.decode.model_year
from @vin.decode.aws_s3_bucket
files = ('Maxs_ModelYear_Data.csv')
file_format =(format_name = vin.decode.comma_sep_oneheadrow);

--there's a typo in the stage name here. Remember that AWS is case-sensitive and fix the file name
COPY INTO vin.decode.manuf_plants
from @vin.decode.aws_s3_bucket
files = ('Maxs_ManufPlants_Data.csv')
file_format =(format_name = vin.decode.comma_sep_oneheadrow);

--there's one table left to load, and one file left to be loaded. 
--figure out what goes in each of the <bracketed> areas to make the final load
COPY INTO VIN.DECODE.MAKE_MODEL_VDS
from @vin.decode.aws_s3_bucket
files = ('Maxs_MMVDS_Data.csv')
file_format =(format_name=vin.decode.comma_sep_oneheadrow);



show stages;-- as name suggests it shows all stages created

LIST @vin.decode.aws_s3_bucket; -- and this query will help you to list all files in stage

--s3://uni-cmcw/Maxs_MMVDS_Data.csv

===============================================================


---------------------------------SNF

--FUNCTION
One way to encapsulate logic is to create a function. This function needs to be SECURE so you can share it. So when you type "CREATE FUNCTION" be sure to add the word "SECURE", as in "CREATE SECURE FUNCTION"

To create a function:

Give the function a name 
Tell the function what information you will be passing into it. 
Tell the function what type of information you expect it to pass back to you (Return). 


--create a variable and set the value
set sample_vin = 'SAJAJ4FX8LCP55916';

--check to make sure you set the variable above
select $sample_vin;

--parse the vin into it's important pieces
SELECT $sample_vin as VIN
  , LEFT($sample_vin,3) as WMI
  , SUBSTR($sample_vin,4,5) as VDS
  , SUBSTR($sample_vin,10,1) as model_year_code
  , SUBSTR($sample_vin,11,1) as plant_code
;

-- This code must be run in the same worksheet (session) as the [set sample_vin =] statement, otherwise the variable will not 'exist'
select VIN
, manuf_name
, vehicle_type
, make_name
, plant_name
, model_year_name as model_year
, model_name
, desc1
, desc2
, desc3
, desc4
, desc5
, engine
, drive_type
, transmission
, mpg
from
  ( SELECT $sample_vin as VIN
  , LEFT($sample_vin,3) as WMI
  , SUBSTR($sample_vin,4,5) as VDS
  , SUBSTR($sample_vin,10,1) as model_year_code
  , SUBSTR($sample_vin,11,1) as plant_code
  ) vin
JOIN vin.decode.wmi_to_manuf w 
    ON vin.wmi = w.wmi
JOIN vin.decode.manuf_to_make m
    ON w.manuf_id=m.manuf_id
JOIN vin.decode.manuf_plants p
    ON vin.plant_code=p.plant_code
    AND m.make_id=p.make_id
JOIN vin.decode.model_year y
    ON vin.model_year_code=y.model_year_code
JOIN vin.decode.make_model_vds vds
    ON vds.vds=vin.vds 
    AND vds.make_id = m.make_id;



--This will get the outline of the function ready to go
--notice that we added "or replace" and "secure" to this code that was not shown in the screenshot
create or replace secure function vin.decode.parse_and_enhance_vin(this_vin varchar(25))
returns table (
    VIN varchar(25)
    , manuf_name varchar(25)
    , vehicle_type varchar(25)
    , make_name varchar(25)
    , plant_name varchar(25)
    , model_year varchar(25)
    , model_name varchar(25)
    , desc1 varchar(25)
    , desc2 varchar(25)
    , desc3 varchar(25)
    , desc4 varchar(25)
    , desc5 varchar(25)
    , engine varchar(25)
    , drive_type varchar(25)
    , transmission varchar(25)
    , mpg varchar(25)
)
as $$

--your sql logic goes here

select VIN
, manuf_name
, vehicle_type
, make_name
, plant_name
, model_year_name as model_year
, model_name
, desc1
, desc2
, desc3
, desc4
, desc5
, engine
, drive_type
, transmission
, mpg
from
  ( SELECT this_vin as VIN
  , LEFT(this_vin,3) as WMI
  , SUBSTR(this_vin,4,5) as VDS
  , SUBSTR(this_vin,10,1) as model_year_code
  , SUBSTR(this_vin,11,1) as plant_code
  ) vin
JOIN vin.decode.wmi_to_manuf w 
    ON vin.wmi = w.wmi
JOIN vin.decode.manuf_to_make m
    ON w.manuf_id=m.manuf_id
JOIN vin.decode.manuf_plants p
    ON vin.plant_code=p.plant_code
    AND m.make_id=p.make_id
JOIN vin.decode.model_year y
    ON vin.model_year_code=y.model_year_code
JOIN vin.decode.make_model_vds vds
    ON vds.vds=vin.vds 
    AND vds.make_id = m.make_id
 
$$;

--here we have changed $sample_vin to this_vin in sql logic as we do not need local variable and Vin is passed to function 

--In each function call below, we pass in a different VIN as THIS_VIN
select *
from table(vin.decode.PARSE_AND_ENHANCE_VIN('SAJAJ4FX8LCP55916'));

select *
from table(vin.decode.PARSE_AND_ENHANCE_VIN('19UUB2F34LA001631'));
 
select *
from table(vin.decode.PARSE_AND_ENHANCE_VIN('5UXCR6C0XL9C77256'));

--------------------------------------------------------------------------------------------------------------------------

------------------------

--QUESTION

Since your ACME account and your ADU account are on two different clouds, how can they share so easily with one another?

--ANS

--Snowflake will create a new database called SNOWFLAKE$GDS and use it to automate the replication needed.

--QUESTION

--How to Load a File of 3 Columns into a Table of 18 Columns?
 
--ANS

--Did you notice that the file we are loading only has 3 columns or fields? The VIN, the Exterior Colors and the Interior Colors are the columns in our file. The other columns in the table will be empty until we use Max's DECODE functionality to populate them. 
--
--To load our file into our table, we'll need a new File Format with two new properties. 
--
--We'll replace the SKIP_HEADER property with a PARSE_HEADER property. This will tell Snowflake to look at that first row and use it to figure out the column names. 
--
--We'll also add a ERROR_ON_COLUMN_COUNT_MISMATCH property. By setting this property FALSE, we'll be telling Snowflake that it's fine if the file has 3 columns but the table has 18. 

-- This file format will allow the 3 column file to be loaded into an 18 column table
-- By parsing the header, Snowflake can infer the column names
CREATE FILE FORMAT util_db.public.CSV_COL_COUNT_DIFF 
type = 'CSV' 
field_delimiter = ',' 
record_delimiter = '\n' 
field_optionally_enclosed_by = '"'
trim_space = TRUE
error_on_column_count_mismatch = FALSE
parse_header = TRUE;


-- With a parsed header, Snowflake can MATCH BY COLUMN NAME during the COPY INTO
copy into stock.unsold.lotstock
from @stock.unsold.aws_s3_bucket/Lotties_LotStock_Data.csv
file_format = (format_name = util_db.public.csv_col_count_diff)
match_by_column_name='CASE_INSENSITIVE';


--after data listing from Max of ADU to ACME 

--we can use function via acme account without having access to logic 

--If ACME Can't see the tables and their data, how can they run the function that uses those tables and their data!



select * 
from table(ADU_VIN.DECODE.PARSE_AND_ENHANCE_VIN('5UXCR6C0XL9C77256'));

alter database VIN_PARSE__ENHANCE;

ALTER DATABASE IF EXISTS VIN_PARSE__ENHANCE RENAME TO ADU_VIN;


--A simple select from Lot Stock (choose any VIN from the LotStock table)
select * 
from stock.unsold.lotstock
where vin = '5J8YD4H86LL013641';

-- here we use ls for lotstock table and pf for parse function
-- this more complete statement lets us combine the data already in the table 
-- with the data returned from the parse function
select ls.vin, ls.exterior, ls.interior, pf.*
from
(select * 
from table(ADU_VIN.DECODE.PARSE_AND_ENHANCE_VIN('5J8YD4H86LL013641'))
) pf
join stock.unsold.lotstock ls
where pf.vin = ls.vin;
;

-- We can use a local (session) variable to make it easier to change the VIN we are trying to enhance
set my_vin = '5J8YD4H86LL013641';

select $my_vin;
select ls.vin, pf.manuf_name, pf.vehicle_type
        , pf.make_name, pf.plant_name, pf.model_year
        , pf.desc1, pf.desc2, pf.desc3, pf.desc4, pf.desc5
        , pf.engine, pf.drive_type, pf.transmission, pf.mpg
from stock.unsold.lotstock ls
join 
    (   select 
          vin, manuf_name, vehicle_type
        , make_name, plant_name, model_year
        , desc1, desc2, desc3, desc4, desc5
        , engine, drive_type, transmission, mpg
        from table(ADU_VIN.DECODE.PARSE_AND_ENHANCE_VIN($my_vin))
    ) pf
on pf.vin = ls.vin;


-- We're using "s" for "source." The joined data from the LotStock table and the parsing function will be a source of data for us. 
-- We're using "t" for "target." The LotStock table is the target table we want to update.
 
update stock.unsold.lotstock t
set manuf_name = s.manuf_name
, vehicle_type = s.vehicle_type
, make_name = s.make_name
, plant_name = s.plant_name
, model_year = s.model_year
, desc1 = s.desc1
, desc2 = s.desc2
, desc3 = s.desc3
, desc4 = s.desc4
, desc5 = s.desc5
, engine = s.engine
, drive_type = s.drive_type
, transmission = s.transmission
, mpg = s.mpg
from 
(
    select ls.vin, pf.manuf_name, pf.vehicle_type
        , pf.make_name, pf.plant_name, pf.model_year
        , pf.desc1, pf.desc2, pf.desc3, pf.desc4, pf.desc5
        , pf.engine, pf.drive_type, pf.transmission, pf.mpg
    from stock.unsold.lotstock ls
    join 
    (   select 
          vin, manuf_name, vehicle_type
        , make_name, plant_name, model_year
        , desc1, desc2, desc3, desc4, desc5
        , engine, drive_type, transmission, mpg
        from table(ADU_VIN.DECODE.PARSE_AND_ENHANCE_VIN($my_vin))
    ) pf
    on pf.vin = ls.vin
) s
where t.vin = s.vin;


ü•ã Setting a Variable with a SQL Query

-- We can count the number of rows in the LotStock table that have not yet been updated.  
 
set row_count = (select count(*) 
                from stock.unsold.lotstock
                where manuf_name is null);

select $row_count;



--------------------------------------------------------------------------------

--CONTROL FLOW


üìì A Look at a SQL Scripting Block
Words like DECLARE, BEGIN, END, FOR are for "control of flow". They allow you to dictate which statements will take place in a certain order and be run one after another. 

The code block below will allow you to update all the remaining LotStock rows by clicking RUN just one more time.

Till now we have updated only three.

Before running the code block below, look at the images below to see what each part is doing. 


When you run the code block, notice how slow it runs!! This is because it is updating one row each time it completes a loop.

Snowflake was originally optimized for bulk loading and bulk updating.


ü•ã Combining the Table Data with the Function Data
copy command

-- This scripting block runs very slow, but it shows how blocks work for people who are new to using them
DECLARE
    update_stmt varchar(2000);
    res RESULTSET;
    cur CURSOR FOR select vin from stock.unsold.lotstock where manuf_name is null;
BEGIN
    OPEN cur;
    FOR each_row IN cur DO
        update_stmt := 'update stock.unsold.lotstock t '||
            'set manuf_name = s.manuf_name ' ||
            ', vehicle_type = s.vehicle_type ' ||
            ', make_name = s.make_name ' ||
            ', plant_name = s.plant_name ' ||
            ', model_year = s.model_year ' ||
            ', desc1 = s.desc1 ' ||
            ', desc2 = s.desc2 ' ||
            ', desc3 = s.desc3 ' ||
            ', desc4 = s.desc4 ' ||
            ', desc5 = s.desc5 ' ||
            ', engine = s.engine ' ||
            ', drive_type = s.drive_type ' ||
            ', transmission = s.transmission ' ||
            ', mpg = s.mpg ' ||
            'from ' ||
            '(       select ls.vin, pf.manuf_name, pf.vehicle_type ' ||
                    ', pf.make_name, pf.plant_name, pf.model_year ' ||
                    ', pf.desc1, pf.desc2, pf.desc3, pf.desc4, pf.desc5 ' ||
                    ', pf.engine, pf.drive_type, pf.transmission, pf.mpg ' ||
                'from stock.unsold.lotstock ls ' ||
                'join ' ||
                '(   select' || 
                '     vin, manuf_name, vehicle_type' ||
                '    , make_name, plant_name, model_year ' ||
                '    , desc1, desc2, desc3, desc4, desc5 ' ||
                '    , engine, drive_type, transmission, mpg ' ||
                '    from table(ADU_VIN.DECODE.PARSE_AND_ENHANCE_VIN(\'' ||
                  each_row.vin || '\')) ' ||
                ') pf ' ||
                'on pf.vin = ls.vin ' ||
            ') s ' ||
            'where t.vin = s.vin;';
        res := (EXECUTE IMMEDIATE :update_stmt);
    END FOR;
    CLOSE cur;   
END;


https://docs.snowflake.com/en/sql-reference-snowflake-scripting

Most people put scripting blocks into Stored Procedures, which are another way to encapsulate different bits of code. 

Snowflake was designed for loading and updating large record sets with a single statement, not for updating one row at a time, using a FOR LOOP. 
There are more efficient ways to achieve the result we achieved above, but this lesson's example allowed you to see how each part became a building block for the next. 

-------------------------------------------------------------------


Badge 3: Data Application Builders Workshop
Almost like a full-stack developer bootcamp, this course covers a wide array of technologies that will help you build applications that use Snowflake as a back end. Including: Streamlit (Python), Streamlit-in-Snowflake, REST APIs, and much more.


--------------------------------------------------------------------
DABW
Yay123456
-------------------------------------------------------------------- 

--Create the Internal Stage and Load the File Into It
 
--'@"SMOOTHIES"."PUBLIC"."MY_UPLOADED_FILES"/fruits_available_for_smoothies.txt'

copy into SMOOTHIES.PUBLIC.FRUIT_OPTIONS
from @SMOOTHIES.PUBLIC.MY_UPLOADED_FILES
files = ('fruits_available_for_smoothies.txt')
file_format = (format_name = SMOOTHIES.PUBLIC.TWO_HEADERROW_PCT_DELIM)
on_error = abort_statement
validation_mode = return_errors --this will return error
purge = true;

--Query the Not-Yet-Loaded Data Using the File Format

SELECT $1, $2, $3, $4, $5
FROM @<stage_name>/<file_name>
(FILE_FORMAT => <file_format_name>)


SELECT $1, $2
FROM '@"SMOOTHIES"."PUBLIC"."MY_UPLOADED_FILES"/fruits_available_for_smoothies.txt'
(FILE_FORMAT => SMOOTHIES.PUBLIC.TWO_HEADERROW_PCT_DELIM);

--TRANSFORMING DATA DURING A LOAD

--Snowflake supports transforming data while loading it into a table using the COPY INTO <table> command, dramatically simplifying your ETL pipeline for basic transformations. This feature helps you avoid the use of temporary tables to store pre-transformed data when reordering columns during a data load. This feature applies to both bulk loading and Snowpipe.

--The COPY command supports:

--Column reordering, column omission, and casts using a SELECT statement. There is no requirement for your data files to have the same number and ordering of columns as your target table.

--The ENFORCE_LENGTH | TRUNCATECOLUMNS option, which can truncate text strings that exceed the target column length.

--Transforming CSV data


--Load a subset of table data

--Load a subset of data into a table. For any missing columns, Snowflake inserts the default values. The following example loads data from columns 1, 2, 6, and 7 of a staged CSV file:

copy into home_sales(city, zip, sale_date, price)
   from (select t.$1, t.$2, t.$6, t.$7 from @mystage/sales.csv.gz t)
   FILE_FORMAT = (FORMAT_NAME = mycsvformat);
   
--Reorder CSV columns during a load
--The following example reorders the column data from a staged CSV file before loading it into a table. Additionally, the COPY statement uses the SUBSTR , SUBSTRING function to remove the first few characters of a string before inserting it:

copy into home_sales(city, zip, sale_date, price)
   from (select SUBSTR(t.$2,4), t.$1, t.$5, t.$4 from @mystage t)
   FILE_FORMAT = (FORMAT_NAME = mycsvformat);
   
--Load semi-structured data into separate columns

--The following example loads repeating elements from a staged semi-structured file into separate table columns with different data types.

--This example loads the following semi-structured data into separate columns in a relational table, with the location object values loaded into a VARIANT column and the remaining values loaded into relational columns:

-- Sample data:
{"location": {"city": "Lexington","zip": "40503"},"dimensions": {"sq_ft": "1000"},"type": "Residential","sale_date": "4-25-16","price": "75836"},
{"location": {"city": "Belmont","zip": "02478"},"dimensions": {"sq_ft": "1103"},"type": "Residential","sale_date": "6-18-16","price": "92567"},
{"location": {"city": "Winchester","zip": "01890"},"dimensions": {"sq_ft": "1122"},"type": "Condo","sale_date": "1-31-16","price": "89921"}


-- Create an internal stage with the file type set as JSON.
 CREATE OR REPLACE STAGE mystage
   FILE_FORMAT = (TYPE = 'json');

 -- Stage a JSON data file in the internal stage.
 PUT file:///tmp/sales.json @mystage;

 -- Query the staged data. The data file comprises three objects in NDJSON format.
 SELECT t.$1 FROM @mystage/sales.json.gz t;

 +------------------------------+
 | $1                           |
 |------------------------------|
 | {                            |
 |   "dimensions": {            |
 |     "sq_ft": "1000"          |
 |   },                         |
 |   "location": {              |
 |     "city": "Lexington",     |
 |     "zip": "40503"           |
 |   },                         |
 |   "price": "75836",          |
 |   "sale_date": "2022-08-25", |
 |   "type": "Residential"      |
 | }                            |
 | {                            |
 |   "dimensions": {            |
 |     "sq_ft": "1103"          |
 |   },                         |
 |   "location": {              |
 |     "city": "Belmont",       |
 |     "zip": "02478"           |
 |   },                         |
 |   "price": "92567",          |
 |   "sale_date": "2022-09-18", |
 |   "type": "Residential"      |
 | }                            |
 | {                            |
 |   "dimensions": {            |
 |     "sq_ft": "1122"          |
 |   },                         |
 |   "location": {              |
 |     "city": "Winchester",    |
 |     "zip": "01890"           |
 |   },                         |
 |   "price": "89921",          |
 |   "sale_date": "2022-09-23", |
 |   "type": "Condo"            |
 | }                            |
 +------------------------------+

 -- Create a target table for the data.
 CREATE OR REPLACE TABLE home_sales (
   CITY VARCHAR,
   POSTAL_CODE VARCHAR,
   SQ_FT NUMBER,
   SALE_DATE DATE,
   PRICE NUMBER
 );

 -- Copy elements from the staged file into the target table.
 COPY INTO home_sales(city, postal_code, sq_ft, sale_date, price)
 FROM (select
 $1:location.city::varchar,
 $1:location.zip::varchar,
 $1:dimensions.sq_ft::number,
 $1:sale_date::date,
 $1:price::number
 FROM @mystage/sales.json.gz t);

 -- Query the target table.
 SELECT * from home_sales;

+------------+-------------+-------+------------+-------+
| CITY       | POSTAL_CODE | SQ_FT | SALE_DATE  | PRICE |
|------------+-------------+-------+------------+-------|
| Lexington  | 40503       |  1000 | 2022-08-25 | 75836 |
| Belmont    | 02478       |  1103 | 2022-09-18 | 92567 |
| Winchester | 01890       |  1122 | 2022-09-23 | 89921 |
+------------+-------------+-------+------------+-------+


--Flatten semi-structured data

--FLATTEN is a table function that produces a lateral view of a VARIANT, OBJECT, or ARRAY column. Using the sample data from Load semi-structured Data into Separate Columns, create a table with a separate row for each element in the objects.

-- Create an internal stage with the file delimiter set as none and the record delimiter set as the new line character
create or replace stage mystage
  file_format = (type = 'json');

-- Stage a JSON data file in the internal stage with the default values
put file:///tmp/sales.json @mystage;

-- Create a table composed of the output from the FLATTEN function
create or replace table flattened_source
(seq string, key string, path string, index string, value variant, element variant)
as
  select
    seq::string
  , key::string
  , path::string
  , index::string
  , value::variant
  , this::variant
  from @mystage/sales.json.gz
    , table(flatten(input => parse_json($1)));

  select * from flattened_source;

+-----+-----------+-----------+-------+-------------------------+-----------------------------+
| SEQ | KEY       | PATH      | INDEX | VALUE                   | ELEMENT                     |
|-----+-----------+-----------+-------+-------------------------+-----------------------------|
| 1   | location  | location  | NULL  | {                       | {                           |
|     |           |           |       |   "city": "Lexington",  |   "location": {             |
|     |           |           |       |   "zip": "40503"        |     "city": "Lexington",    |
|     |           |           |       | }                       |     "zip": "40503"          |
|     |           |           |       |                         |   },                        |
|     |           |           |       |                         |   "price": "75836",         |
|     |           |           |       |                         |   "sale_date": "2017-3-5",  |
|     |           |           |       |                         |   "sq__ft": "1000",         |
|     |           |           |       |                         |   "type": "Residential"     |
|     |           |           |       |                         | }                           |
...
| 3   | type      | type      | NULL  | "Condo"                 | {                           |
|     |           |           |       |                         |   "location": {             |
|     |           |           |       |                         |     "city": "Winchester",   |
|     |           |           |       |                         |     "zip": "01890"          |
|     |           |           |       |                         |   },                        |
|     |           |           |       |                         |   "price": "89921",         |
|     |           |           |       |                         |   "sale_date": "2017-3-21", |
|     |           |           |       |                         |   "sq__ft": "1122",         |
|     |           |           |       |                         |   "type": "Condo"           |
|     |           |           |       |                         | }                           |
+-----+-----------+-----------+-------+-------------------------+-----------------------------+
 
-------------------------------------------------- SNF

SELECT $1, $2
FROM '@"SMOOTHIES"."PUBLIC"."MY_UPLOADED_FILES"/fruits_available_for_smoothies.txt'
(FILE_FORMAT => SMOOTHIES.PUBLIC.TWO_HEADERROW_PCT_DELIM);


--changing the order by columns

copy into SMOOTHIES.PUBLIC.FRUIT_OPTIONS
from ( select $2 as FRUIT_ID, $1 as FRUIT_NAME 
FROM '@"SMOOTHIES"."PUBLIC"."MY_UPLOADED_FILES"/fruits_available_for_smoothies.txt')
file_format = (format_name = SMOOTHIES.PUBLIC.TWO_HEADERROW_PCT_DELIM)
on_error = abort_statement
purge = true; --purge true will remove original file

--------------------------------------------------------SiS stearmlit

# Import python packages
import streamlit as st
from snowflake.snowpark.context import get_active_session

# Write directly to the app
st.title(":cup_with_straw: Customize Your Smoothie! :cup_with_straw:")
st.write(
    """Choose the fruit you want in your Smothie!"""
)


option = st.selectbox(
    "What is your favorite fruit?",
    ("Banana", "Strawberries", "Peaches"),
)

st.write("You selected:", option)

session = get_active_session()
my_dataframe = session.table("smoothies.public.fruit_options")
st.dataframe(data=my_dataframe, use_container_width=True)


----------------Stearmlit APP DEV

# Import python packages
import streamlit as st
from snowflake.snowpark.context import get_active_session
from snowflake.snowpark.functions import col

# Write directly to the app
st.title(":cup_with_straw: Customize Your Smoothie! :cup_with_straw:")
st.write(
    """Choose the fruit you want in your Smothie!"""
)

name_on_order = st.text_input("Name of Smoothie")
st.write("The name on your Smooothie will be :", name_on_order)


session = get_active_session()
my_dataframe = session.table("smoothies.public.fruit_options").select(col('FRUIT_NAME'))
#st.dataframe(data=my_dataframe, use_container_width=True)

ingredients_list = st.multiselect('Choose up to 5 ingredients:', my_dataframe)


if ingredients_list:
    
    ingredients_string = ''

    for fruit_chosen in ingredients_list:
        ingredients_string += fruit_chosen + ' '

    #st.write(ingredients_string)
    
    my_insert_stmt = """ insert into smoothies.public.orders(ingredients,name_on_order)
            values ('""" + ingredients_string + """','"""+name_on_order+ """')"""   
    
    #st.write(my_insert_stmt)
    #st.stop() # good to have in checking before Sis enter data in DB

    time_to_insert = st.button('Submit Order') # this will not case each time you click on fruit to enter in row

    if time_to_insert:
        session.sql(my_insert_stmt).collect()
        
        st.success('Your Smoothie is ordered!', icon="‚úÖ")
--------------------------------------------------------------------------

------------------------------Second app  Pending Smoothie order
# Import python packages
import streamlit as st
from snowflake.snowpark.context import get_active_session
from snowflake.snowpark.functions import col

# Write directly to the app
st.title(":cup_with_straw: Pending Smoothie order! :cup_with_straw:")
st.write(
    """Orders that needs to be filled."""
)

session = get_active_session()

#my_dataframe = session.table("smoothies.public.orders")

my_dataframe = session.table("smoothies.public.orders").filter(col("NAME_ON_ORDER") != 'NONE').collect()

st.write(my_dataframe)

----------------------------------------------------

Badge 2: Collaboration, Marketplace & Cost Estimation Workshop

Url: https://achieve.snowflake.com/d162c284-ae4b-408d-977c-3c9608f573a5#gs.f0yvkc

----------------------------------------------------

-------------------------------------SNF
--Batch 5 Data Engineering
‚Ä¢	Username: JAI10 
‚Ä¢	Dedicated Login URL: https://pqxyrdr-gj64664.snowflakecomputing.com 


Run data analysis and build visualizations

Build or train a machine learning model

Load data, build a data pipeline or migrate an existing warehouse

Build or distribute an application with Snowflake

List or buy data from the Snowflake marketplace

Use Snowflake as a data warehouse and data lake


create database  AGS_GAME_AUDIENCE;

drop schema AGS_GAME_AUDIENCE.PUBLIC;

create schema AGS_GAME_AUDIENCE.RAW;

create or replace TABLE AGS_GAME_AUDIENCE.RAW.GAMES_LOGS (
	RAW_LOG VARIANT
);

list @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE;
--here it can list all files in stages 
--this is example of AWS S3 bucket storage

create or replace file format AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS
    type = 'JSON'
    strip_outer_array = true -- this will result in data in different rows
    ;
select $1,$2,$3,$4,$5 from
   '@"AGS_GAME_AUDIENCE"."RAW"."UNI_KISHORE"/kickoff/DNGW_Sample_from_Agnies_Game.json';
   --with out file format

select $1 from
   '@"AGS_GAME_AUDIENCE"."RAW"."UNI_KISHORE"/kickoff/DNGW_Sample_from_Agnies_Game.json'
   (file_format => "FF_JSON_LOGS");

copy into AGS_GAME_AUDIENCE.RAW.GAMES_LOGS
from @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE/kickoff
file_format = (format_name =FF_JSON_LOGS);
--if you have more than one file in the stage path then all files will get loaded into table

--A COPY INTO statement like the one shown above will load EVERY file in the folder if more than one file is there, and the file name is not specified.

list @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE;

truncate table  AGS_GAME_AUDIENCE.RAW.GAMES_LOGS;

select * from AGS_GAME_AUDIENCE.RAW.GAMES_LOGS;

--SELECT statement that separates every field in the RAW_LOG column into its own column of the SELECT results.
select 

RAW_LOG:user_login::text as USER_LOGIN,
RAW_LOG:user_event::text  as USER_EVENT,
RAW_LOG:datetime_iso8601::TIMESTAMP_NTZ as DATETIME_ISO8601,
RAW_LOG:agent::text as AGENT,

RAW_LOG

from AGS_GAME_AUDIENCE.RAW.GAMES_LOGS;

--here "::" is use for casting datatype while ":" is used to get attributes

--creating above logs in view

create view LOGS
as
(
select 

    RAW_LOG:user_login::text as USER_LOGIN,
    RAW_LOG:user_event::text  as USER_EVENT,
    RAW_LOG:datetime_iso8601::TIMESTAMP_NTZ as DATETIME_ISO8601,
    RAW_LOG:agent::text as AGENT,
    RAW_LOG

from AGS_GAME_AUDIENCE.RAW.GAMES_LOGS
);


select * from LOGS;
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

EXECUTE IMMEDIATE
Executes a string that contains a SQL statement or a Snowflake Scripting statement.

https://docs.snowflake.com/en/developer-guide/snowflake-scripting/index

Snowflake Scripting Developer Guide->

Understanding blocks in Snowflake Scripting

DECLARE
  -- (variable declarations, cursor declarations, etc.) ...
  --Variables --Cursors --RESULTSETs --Exceptions
BEGIN
  -- (Snowflake Scripting and SQL statements) ...
EXCEPTION
  -- (statements for handling exceptions) ...
END;

--Snowflake Transient Tables can act as temporary staging areas for loading and transforming data before moving it to permanent tables.

-- What Is the Difference Between Temporary and Transient Tables in Snowflake?
Snowflake Temp table exists only within the current session. Automatically dropped when the session ends	
Persist beyond the current session and are available to all users with the necessary credentials until they are dropped

-- What Is the Difference Between CTE and Temp Table in Snowflake?
Snowflake Temporary tables create an object in the instance that can be queried repeatedly during the session. They are primarily used when you have a large dataset that
you want to refer to multiple times for a short period.
Snowflake CTEs are named subqueries defined within the WITH clause of a Snowflake SQL statement. They are used to break up complex queries into smaller, more manageable 
chunks and do not create any physical objects.

A simple block only requires the keywords BEGIN and END. For example:

Begin
    CREATE or replace temporary TABLE table_name (
      Time timestamp,
      account_name varchar)
      as (
        select current_timestamp,current_user);
End;


Declare
    radius float;
    surface_area float;
    volumne float;
    Begin
        radius := 2 ;
        surface_area := 4*pi()*(radius)*(radius);
        Return surface_area; 	--return a values , if retrun is not specified than output is anonymous block with null
    End;

--Stored procedure example
create or replace procedure surface_area()
returns float
language sql
as 
$$
Declare
    radius float;
    surface_area float;
    begin
        radius := 2 ;
        surface_area := 4*pi()*(radius)*(radius);
        Return surface_area; --return a values , if retrun is not specified than output is anonymous block with null
    End;
$$
;

call surface_area();

--------------------------------------------------------------------------------------------------------
Snowflake offers several major advantages over SQL Server, particularly for data warehousing and analytics:

1. Scalability:
   - Snowflake: Automatically scales storage and compute resources independently, allowing for seamless handling of varying workloads and data volumes
   - SQL Server: Requires manual scaling and infrastructure management, which can be more complex and less flexible.

2. Performance:
   - Snowflake: Optimized for read-heavy analytical queries with advanced compression techniques and columnar storage, leading to faster query performance
   - SQL Server: While performant, it often requires manual tuning and optimization to achieve similar results.

3. Maintenance and Management:
   - Snowflake: Fully managed service with automatic maintenance, updates, and backups, reducing administrative overhead
   - SQL Server: Requires ongoing maintenance, updates, and backups, which can be time-consuming and resource-intensive.

4. Cost Efficiency:
   - Snowflake: Pay-as-you-go pricing model, where you only pay for the resources you use, potentially leading to cost savings
   - SQL Server: Typically involves upfront costs for hardware and software, along with ongoing maintenance expenses.

5. Data Sharing and Collaboration:
   - Snowflake: Built-in data sharing capabilities that allow secure and easy sharing of data across different organizations and platforms
   - SQL Server: Data sharing is possible but often requires more complex configurations and integrations.

6. Multi-Cloud Support:
   - Snowflake: Supports multiple cloud platforms (AWS, Azure, Google Cloud), providing flexibility and reducing vendor lock-in
   - SQL Server: Primarily designed for on-premises use, though it can be deployed on Azure for cloud capabilities.

7. Security and Compliance:
   - Snowflake: Offers robust security features, including encryption, role-based access control, and compliance with various industry standards
   - SQL Server: Also provides strong security features, but managing and maintaining these can be more complex in an on-premises environment.


----------------------------------------------------------------------
SNOWFLAKE INFORMATION SCHEMA

The Snowflake Information Schema (aka ‚ÄúData Dictionary‚Äù) consists of a set of system-defined views and table functions that provide extensive metadata information about 
the objects created in your account. The Snowflake Information Schema is based on the SQL-92 ANSI Information Schema, but with the addition of views and functions that
are specific to Snowflake.

The Information Schema is implemented as a schema named INFORMATION_SCHEMA that Snowflake automatically creates in every database in an account.

What is INFORMATION_SCHEMA?
Each database created in your account automatically includes a built-in, read-only schema named INFORMATION_SCHEMA. The schema contains the following objects:

Views for all the objects contained in the database, as well as views for account-level objects (i.e. non-database objects such as roles, warehouses, and databases)

Table functions for historical and usage data across your account.

General usage notes
SELECT table_name, comment FROM testdb.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'PUBLIC' ... ;

SELECT event_timestamp, user_name FROM TABLE(testdb.INFORMATION_SCHEMA.LOGIN_HISTORY( ... ));

USE DATABASE testdb;

SELECT table_name, comment FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'PUBLIC' ... ;

SELECT event_timestamp, user_name FROM TABLE(INFORMATION_SCHEMA.LOGIN_HISTORY( ... ));

USE SCHEMA testdb.INFORMATION_SCHEMA;

SELECT table_name, comment FROM TABLES WHERE TABLE_SCHEMA = 'PUBLIC' ... ;

SELECT event_timestamp, user_name FROM TABLE(LOGIN_HISTORY( ... ));

------------------------------Get List of all OBJECTS in ACCOUNT

SHOW PROCEDURES IN ACCOUNT;

SHOW USER FUNCTIONS IN ACCOUNT;

Show Stages in account;

show pipes in account;

show databases in account;

SHOW SCHEMAS in account;

Show tables in account;


Both of these will list all objects that your current role has access to. If run as ACCOUNTADMIN, they will return everything.
